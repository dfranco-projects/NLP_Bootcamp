{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "076526f3",
   "metadata": {},
   "source": [
    "# **01 - NLP Tokenization**\n",
    "\n",
    "Now that we’ve covered the basics, let’s dive deeper. Consider that we’ve gathered and cleaned our corpus. The first question to ask is: **How can we represent text so that a computer can understand it?** This is where **tokenization** comes in.\n",
    "\n",
    "Tokenization, also known as text segmentation, is the process of breaking text into smaller chunks, like words or sentences, becoming tokens. It's the first step in turning raw text into something usable for NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05667f0a-2c24-4c7a-828f-70f9382d069e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f883f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Tokenizer</th>\n",
       "      <th>Sentence Split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Whitespace-based Tokenization</td>\n",
       "      <td>['Hi', 'there!', 'How', 'are', 'you', 'doing',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Punctuation-based Tokenization</td>\n",
       "      <td>[' ', '!', ' ', ' ', ' ', ' ', ' ', '?', ' ', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Default/Treebank Tokenizer</td>\n",
       "      <td>['Hi', 'there', '!', 'How', 'are', 'you', 'doi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tweet Tokenizer</td>\n",
       "      <td>['Hi', 'there', '!', 'How', 'are', 'you', 'doi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MWE Tokenizer</td>\n",
       "      <td>['Hi', 'there', '!', 'How', 'are', 'you', 'doi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Word Tokenizer  \\\n",
       "0   Whitespace-based Tokenization   \n",
       "1  Punctuation-based Tokenization   \n",
       "2      Default/Treebank Tokenizer   \n",
       "3                 Tweet Tokenizer   \n",
       "4                   MWE Tokenizer   \n",
       "\n",
       "                                      Sentence Split  \n",
       "0  ['Hi', 'there!', 'How', 'are', 'you', 'doing',...  \n",
       "1  [' ', '!', ' ', ' ', ' ', ' ', ' ', '?', ' ', ...  \n",
       "2  ['Hi', 'there', '!', 'How', 'are', 'you', 'doi...  \n",
       "3  ['Hi', 'there', '!', 'How', 'are', 'you', 'doi...  \n",
       "4  ['Hi', 'there', '!', 'How', 'are', 'you', 'doi...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"Hi there! How are you doing today? Hope you're having a good time at the NLP Bootcamp.\"\n",
    "\n",
    "# Tokenizers\n",
    "whitespace_tokenizer = sentence.split()\n",
    "punctuation_tokenizer = nltk.regexp_tokenize(sentence, pattern=r'\\s|[\\.,!?;\"]')\n",
    "treebank_tokenizer = nltk.word_tokenize(sentence)\n",
    "tweet_tokenizer = nltk.tokenize.TweetTokenizer().tokenize(sentence)\n",
    "mwe_tokenizer = nltk.word_tokenize(sentence)  # Example of simple MWE tokenizer, here we'll keep it same as Treebank\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    'Word Tokenizer': ['Whitespace-based Tokenization', 'Punctuation-based Tokenization', \n",
    "                       'Default/Treebank Tokenizer', 'Tweet Tokenizer', 'MWE Tokenizer'],\n",
    "    'Sentence Split': [\n",
    "        str(whitespace_tokenizer),\n",
    "        str(punctuation_tokenizer),\n",
    "        str(treebank_tokenizer),\n",
    "        str(tweet_tokenizer),\n",
    "        str(mwe_tokenizer)\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
