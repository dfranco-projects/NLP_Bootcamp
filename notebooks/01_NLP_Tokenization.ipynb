{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "076526f3",
   "metadata": {},
   "source": [
    "# **01 - NLP Tokenization**\n",
    "\n",
    "Now that we’ve covered the basics, let’s dive deeper. Consider that we’ve gathered and cleaned our corpus. The first question to ask is: **How can we represent text so that a computer can understand it?** This is where **tokenization** comes in.\n",
    "\n",
    "Tokenization, also known as text segmentation, is the process of breaking text into smaller chunks, like words or sentences, becoming tokens. It's the first step in turning raw text into something usable for NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd16f4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
